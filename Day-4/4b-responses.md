âœ… ADK components imported successfully.
âœ… API key loaded from .env file

================================================================================
DAY 4B: AGENT EVALUATION
================================================================================

ğŸ“š What You'll Learn:
â€¢ Creating test cases interactively in ADK web UI
â€¢ Running systematic evaluations with CLI
â€¢ Understanding evaluation metrics
â€¢ Analyzing and fixing evaluation failures
â€¢ Advanced user simulation concepts

================================================================================
DEMO: Interactive Evaluation with ADK Web UI
================================================================================

ğŸ“ Interactive Evaluation Workflow:

1ï¸âƒ£  CREATE TEST CASES:
   â€¢ Start ADK web UI: adk web
   â€¢ Have a conversation with your agent
   â€¢ Navigate to 'Eval' tab
   â€¢ Click 'Create Evaluation set' and name it
   â€¢ Add current session to the evaluation set

2ï¸âƒ£  RUN EVALUATION:
   â€¢ In the Eval tab, check your test case
   â€¢ Click 'Run Evaluation' button
   â€¢ Review the metrics dialog (response_match and tool_trajectory)
   â€¢ Click 'Start' to run

3ï¸âƒ£  ANALYZE RESULTS:
   â€¢ Green 'Pass': Agent behaved as expected
   â€¢ Red 'Fail': Agent deviated from expected behavior
   â€¢ Hover over results for detailed comparison
   â€¢ View actual vs expected responses and tool calls

ğŸ“Š Understanding Evaluation Metrics:
   â€¢ response_match_score: Measures text similarity (1.0 = perfect match)
   â€¢ tool_trajectory_avg_score: Measures correct tool usage (1.0 = perfect)

ğŸ’¡ Benefits of Interactive Evaluation:
   âœ… Visual feedback on agent behavior
   âœ… Quick iteration during development
   âœ… Easy test case creation from real conversations
   âœ… Immediate comparison of actual vs expected

================================================================================
DEMO: Systematic Evaluation with CLI
================================================================================

ğŸ¯ Why Systematic Evaluation?
   â€¢ Scale beyond manual testing
   â€¢ Automated regression detection
   â€¢ Batch testing multiple scenarios
   â€¢ CI/CD integration for production

ğŸ“ Creating evaluation files...
âœ… Evaluation configuration created!
   Saved to: home_automation_agent/test_config.json

ğŸ“Š Evaluation Criteria:
â€¢ tool_trajectory_avg_score: 1.0 - Requires exact tool usage match
â€¢ response_match_score: 0.8 - Requires 80% text similarity

ğŸ¯ What this evaluation will catch:
âœ… Incorrect tool usage (wrong device, location, or status)
âœ… Poor response quality and communication
âœ… Deviations from expected behavior patterns

âœ… Evaluation test cases created
   Saved to: home_automation_agent/integration.evalset.json

ğŸ§ª Test scenarios:
â€¢ living_room_light_on: Please turn on the floor lamp in the living room
â€¢ kitchen_on_off_sequence: Switch on the main light in the kitchen.

ğŸ“Š Expected results:
â€¢ living_room_light_on: Should pass both criteria
â€¢ kitchen_on_off_sequence: Should pass both criteria

ğŸ’¡ These test cases verify the agent uses correct tools with correct parameters

ğŸš€ Running Evaluation:

   Command: adk eval home_automation_agent home_automation_agent/integration.evalset.json \
              --config_file_path=home_automation_agent/test_config.json \
              --print_detailed_results

ğŸ“Š What the CLI Does:
   1. Loads your agent from the specified directory
   2. Runs each test case from the evalset
   3. Compares actual vs expected:
      â€¢ Final responses (text similarity)
      â€¢ Tool usage (function calls and parameters)
   4. Applies pass/fail thresholds from config
   5. Prints detailed results for each test

ğŸ“ˆ Sample Output Analysis:
   Test Case: living_room_light_on
   âœ… tool_trajectory_avg_score: 1.0/1.0 (PASS)
   âŒ response_match_score: 0.45/0.80 (FAIL)

   Diagnosis:
   â€¢ Tool usage is perfect (correct tool, correct params)
   â€¢ Response quality needs improvement
   â€¢ Fix: Update agent instructions for consistent messaging

âš™ï¸  To actually run the evaluation:
   1. Create agent: adk create home_automation_agent
   2. Copy agent definition to agent.py
   3. Run: adk eval home_automation_agent home_automation_agent/integration.evalset.json \
            --config_file_path=home_automation_agent/test_config.json \
            --print_detailed_results

================================================================================
DEMO: User Simulation (Advanced)
================================================================================

ğŸ¯ The Limitation of Static Tests:
   â€¢ Fixed test cases only cover known scenarios
   â€¢ Real users are unpredictable and varied
   â€¢ Conversations take unexpected turns
   â€¢ Edge cases emerge in production

ğŸ’¡ User Simulation Solution:
   â€¢ Uses LLM to generate dynamic user prompts
   â€¢ Follows a ConversationScenario with goals
   â€¢ Adapts based on agent responses
   â€¢ Discovers edge cases automatically

ğŸ“ How It Works:
   1. Define a ConversationScenario:
      â€¢ User's overall goal
      â€¢ Conversation plan outline
   2. LLM acts as simulated user:
      â€¢ Generates realistic prompts
      â€¢ Maintains conversational context
      â€¢ Adapts to agent behavior
   3. Evaluation runs automatically:
      â€¢ Tests agent's adaptability
      â€¢ Uncovers unexpected failures
      â€¢ More comprehensive coverage

ğŸ“š Learn More:
   â€¢ User Simulation Docs: https://google.github.io/adk-docs/evaluate/user-sim/
   â€¢ Implement ConversationScenario for your agent
   â€¢ Test against dynamic, realistic conversations

================================================================================
EVALUATION BEST PRACTICES
================================================================================

1ï¸âƒ£  BUILD A COMPREHENSIVE TEST SUITE:
   â€¢ Happy path scenarios (basic functionality)
   â€¢ Edge cases (unusual requests)
   â€¢ Error handling (invalid inputs)
   â€¢ Multi-turn conversations
   â€¢ Ambiguous user intents

2ï¸âƒ£  SET APPROPRIATE THRESHOLDS:
   â€¢ tool_trajectory_avg_score:
     - 1.0 for critical operations (safety, financial)
     - 0.8-0.9 for general functionality
   â€¢ response_match_score:
     - 0.9-1.0 for exact wording requirements
     - 0.7-0.8 for semantic equivalence

3ï¸âƒ£  ITERATIVE IMPROVEMENT:
   â€¢ Run evaluations frequently during development
   â€¢ Add test cases when bugs are discovered
   â€¢ Update thresholds based on production needs
   â€¢ Monitor evaluation trends over time

4ï¸âƒ£  PRODUCTION EVALUATION:
   â€¢ Integrate into CI/CD pipeline
   â€¢ Run before deployments
   â€¢ Track metrics over versions
   â€¢ Alert on regression detection

5ï¸âƒ£  ADVANCED CRITERIA (with Google Cloud):
   â€¢ safety_v1: Detect harmful content
   â€¢ hallucinations_v1: Check factual accuracy
   â€¢ custom_criteria: Define domain-specific metrics

================================================================================
SUMMARY
================================================================================

ğŸ¯ Key Takeaways:
âœ… Evaluation is proactive (vs observability is reactive)
âœ… Test both tool usage AND response quality
âœ… Use ADK web UI for development iteration
âœ… Use CLI evaluation for systematic regression testing
âœ… User simulation extends coverage beyond static tests

ğŸ“Š Evaluation Workflow:
1. Create test cases (web UI or synthetic)
2. Define evaluation criteria (config file)
3. Run evaluations (adk eval CLI)
4. Analyze results (detailed output)
5. Fix issues and iterate

ğŸ“š Learn More:
â€¢ ADK Evaluation: https://google.github.io/adk-docs/evaluate/
â€¢ Evaluation Criteria: https://google.github.io/adk-docs/evaluate/criteria/
â€¢ Pytest Integration: https://google.github.io/adk-docs/evaluate/#2-pytest-run-tests-programmatically
â€¢ User Simulation: https://google.github.io/adk-docs/evaluate/user-sim/

ğŸš€ Next Steps:
â€¢ Apply evaluation to your own agents
â€¢ Build comprehensive test suites
â€¢ Integrate into your development workflow
â€¢ Stay tuned for Day 5: Production Deployment!